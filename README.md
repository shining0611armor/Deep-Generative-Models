# ğŸ“ Deep Generative Models (DGM) â€“ Spring 1404 / 2025  
**K. N. Toosi University of Technology**  
**Instructor**: Dr. B. Nasihatkon  
**Course Level**: Masterâ€™s & PhD  
**Head Teaching Assistant and Designer**: Mehran Tamjidi  
 
--- 

## ğŸ“˜ Course Overview  
Welcome to the official repository for the graduate-level course **Deep Generative Models (DGM)**, offered in **Spring 1404 (2025)** at **K. N. Toosi University of Technology**. This course explores the mathematical foundations and modern deep learning techniques underlying generative modeling, with a strong emphasis on probabilistic reasoning, graphical models, and state-of-the-art architectures for image and data generation.

This repository contains all **course materials and homework assignments**, designed to guide students through foundational theory, structured probabilistic models, and cutting-edge generative algorithms using **PyTorch**.

---

## ğŸ“¦ Included Modules & Assignments  

### ğŸ§® Homework 1 â€“ Probability and Statistical Foundations  
Lay the groundwork with key probabilistic and statistical concepts required for understanding generative models:

- ğŸ“ **Modeling Stone Drops in a Cylindrical Well** â€“ Analyze continuous distributions and transformations.  
- ğŸ”— **Joint, Marginal, and Conditional Distributions** â€“ Discrete variable modeling and normalization via the Riemann zeta function.  
- ğŸ“Š **Testing Independence** â€“ Use tabulated PMFs to assess variable relationships.  
- ğŸ“ **Parameter Counting** â€“ Quantify the degrees of freedom in discrete models.  
- ğŸ”„ **Conditional Independence Proofs** â€“ Solidify your grasp of graphical model semantics.  

---

### ğŸ§  Homework 2 â€“ Bayesian Network-Based Generation of Persian Digits  
Implement generative models using **Bayesian Networks**:

- ğŸ–¼ï¸ **Binary Persian Digit Dataset** â€“ Preprocessing and format handling.  
- ğŸ•¸ï¸ **Bayesian Network Construction** â€“ Pixel-wise CPDs using 3-, 8-, and 15-connected structures.  
- ğŸ“Š **Linear Sigmoid CPDs** â€“ Shared parameterization across the image grid.  
- ğŸ’» **Deliverables** â€“ PyTorch implementations with well-documented code and generated digit samples.  

---

### ğŸ” Homework 3 â€“ Variational Inference & Autoregressive Modeling  
Tackle both inference and generation with modern deep learning:

- ğŸ“ **Deriving ELBO** â€“ For fully factorized variational posteriors in Bayes Nets.  
- ğŸ¤– **Lightweight Image GPT** â€“ Autoregressive generation of Persian digit images with resource-efficient techniques.  
- ğŸ§® **PyTorch Implementation** â€“ Modular code with interpretable sampling and inference.  

---

### ğŸŒŒ Homework 4 â€“ Normalizing Flows & Latent Space Manipulation  
Dive into latent-variable generative models and 3D data processing:

- ğŸ¨ **Image Generation with GLOW** â€“ Attribute transfer, noise injection, and embedding-space editing.  
- ğŸŒ€ **3D Point Cloud Denoising** â€“ Apply flow-based denoising to corrupted ModelNet-40C point clouds.  
- ğŸ› ï¸ **End-to-End PyTorch Code** â€“ Focused on reproducibility and experimentation.  

---

### ğŸ”‹ Homework 5 â€“ Energy-Based Models, JEM++, DVAEs & GANs  
Explore advanced generative modeling paradigms through deep energy formulations, discrete representations, and adversarial training:

- âš¡ **Energy-Based Models (EBMs)** â€“ Train scalar-valued networks `E(x)` and `E(x, y)` using Stochastic Langevin Dynamics and contrastive divergence. Compare unconditional and class-conditional sampling on CIFAR-10 and MNIST.  
- ğŸ” **Replay Buffer & Noise Scheduling** â€“ Investigate initialization strategies (random vs. replay buffer) and noise annealing using exponential schedules.  
- ğŸ” **JEM & JEM++** â€“ Turn a classifier into a joint energy model. Use a mix of cross-entropy and generative losses with SGLD. JEM++ adds GMM-based initialization and Proximal SGLD for improved sampling stability.  
- ğŸ”¢ **Discrete Variational Autoencoders (DVAEs)** â€“ Quantize latent space using vector quantization, apply KMeans clustering, and analyze using silhouette scores, confusion matrices, and t-SNE plots.  
- ğŸ‡®ğŸ‡· **GANs for Persian Digits** â€“ Train and compare vanilla GAN and Wasserstein GAN (WGAN) on binary Persian digits. Assess image quality, loss convergence, and sample fidelity.

Each section is implemented using PyTorch and includes visualizations, quantitative evaluations, and reproducible experiment code.

---
## ğŸ› ï¸ Technologies & Tools  
- Python 3.10+  
- PyTorch  
- NumPy / SciPy / Matplotlib  
- Jupyter Notebooks  
- TorchVision & TorchMetrics  

---

## ğŸ¯ Learning Objectives  
By the end of this course, students will be able to:  
- Understand and apply the probabilistic foundations of generative modeling.  
- Design and implement Bayesian networks and variational inference techniques.  
- Construct autoregressive models for structured data generation.  
- Utilize normalizing flows for high-quality image synthesis and latent space exploration.
- Apply energy-based modeling and adversarial frameworks for advanced generation tasks.  

---

## ğŸ“š Academic Integrity  
All work submitted must be your own. Discussions and collaborations are encouraged for conceptual understanding, but any code or write-up must be written individually unless explicitly allowed.  

## ğŸ“« Contact
Feel free to reach out if you have any questions or suggestions:
- **Email**: mehrant.0611@gmail.com
- **GitHub**: [shining0611armor](https://github.com/shining0611armor)



Happy Learning! ğŸ˜Š



---

## ğŸ”– Citation  
If you find the course material useful for academic or educational purposes, please cite the course as:  
```scss
Deep Generative Models (DGM), Dr. B. Nasihatkon, K. N. Toosi University of Technology, Spring 1404 (2025).


